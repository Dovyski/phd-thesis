\chapter{Ethics and privacy}
\label{ch:ethics}

Several contributions of the research presented in this thesis have implications that concern ethics and privacy. Technology has become an essential part of modern life and the advances it brings to different fields, including human-computer interaction and games research, should be guided by ethics and privacy. This chapter provides an overview and discussion regarding how the research and the technology proposed in this thesis touches issues related to both ethics and privacy.

\section{Ethical use of technology}
\label{s:ethics-ethical}

\textcite{mason1995applying} mentions that the facts of an ethical situation can be summarized by four factors. The first one is the identification of the moral agent, which is the one bringing the technology-induced change. The second relates to the available courses of action that the moral agent can undertake. It is not always possible, or even viable, to choose more than one course of action. Consequentially it must be selected according to the best interests of all parties involved. Additionally a course of action is bound to have consequences, which can be irreversible. In that light, the third factor emerges, which is the delineation of the results that are expected to occur if each act is taken. A proper delimitation of results make it clear for the involved parties how to measure impact and implications of an act. Finally the fourth factor is the identification of the stakeholders who will be affected by the consequences of the acts.

One of the main goals of the technology developed in this thesis is a non-obtrusive form of emotion detection. Given that a person has agreed to have a user-tailored model of him/herself created, i.e. play the calibration games while being filmed, any moral agent, i.e. researcher or company, is then able to use such data freely and unrestrictedly. After the model has been trained, the person used to train said model can be indefinitely surveyed in a context of gaming. Once trained, the model can be easily transferred to another moral agent, e.g. another institution or company, and used in a later time. Even though the proposed method is constrained by a gaming context, it can still be widely used. If the person in question, who is the stakeholder of the process, was not properly and clearly informed about who the moral agents are and the delineation of the results expected from the use of his/her model, a probable ethical issue exists.

An ethical issue is said to arise whenever one party in pursuit of its goals engages in behavior that materially affects the ability of another party to pursue its goals \parencite{mason1995applying}. One could claim that sharing a person's user-tailored model among institutions/companies is not materially affecting the person. Additionally, people are more prepared to accept potentially invasive technology if they consider that its benefits outweigh potential risks \parencite{ladd1991computers}. However, one of the moral agents might be a game developer company using the model to detect the emotional state of a person in order to maximize the selling of in-game goods. In that case, the act might be materially affecting the person. That is clearly an ethical issue if the person was never made aware of such possible use of his/her model. As previously mentioned, the facts of an ethical situation must be clear, otherwise obscure information about courses of action, delimitation of results and even who the moral agents are might lead stakeholders into making poor judgments regarding ethics and privacy.

Another implication of non-obtrusive technologies is how it influences the ability of user to decline the propagation of any information. In the context of games research, for instance, if a subject is answering a questionnaire about a game being played, it is completely plausible to assume that the subject could deliberately lie about the answers. Subjects might even decline to answer a particular question about emotions if they feel uncomfortable, for instance. If the method proposed by this thesis is being used to detect emotional states and assuming that subjects have previously agreed to have a user-tailored model of themselves created, subjects have no option to decline to answer a query about emotions. A researcher might have a previously trained model of a subject, e.g. from an old experiment, which can be used again for the same subject, however in a different context. The method proposed in this thesis can be adapted to be trained on data from a group instead of an individual, i.e. group model instead of a user-tailored model. In that case, the trained model could be applied to any person (or subject) without them having to play the calibration games. It is plausible to believe that such configuration of the method could be used by companies to survey player's emotional responses to a particular game. A company could, for instance, apply the method to on-line videos, e.g. ``Let's play" videos on YouTube, to gather unsolicited emotional data. If the videos are freely available, does it mean such use of the method is ethical? Was the person in the video thinking about having his/her emotions automatically detected by a software when he/she made the video?

The technology proposed by this thesis has several limitations and constrains, however it can be extended and improved to broaden its accuracy and usage. It has moral and ethical implications that should be discussed by all stakeholders involved, making the facts of any ethical situation completely clear and understood by all involved.

\section{Privacy and personal data}
\label{s:ethics-privacy}

Discussion about privacy in the field of human-computer interaction are common and there is a clear indication that HCI tools must not invade user's privacy \parencite{pantic2003toward}. Any tool's capacity to monitor and concentrate information about somebody's behavior must not be misused. The technology presented in this thesis significantly relates to privacy. As defined by \textcite{culnan2000protecting}, privacy is the ability of the individual to control the terms under which personal information is acquired and used. When a system collects personal information, which is the case of the method in this thesis, information privacy becomes an issue. \textcite{stone1983field} defines information privacy as the ability of the individual to personally control information about one's self.

In the context of this thesis, information privacy relates to how a person controls the digital data collected from him/herself, e.g. video recordings and the user-tailored model. As previously mentioned, the technology presented in this thesis has moral and ethical implications, which leads to information privacy implications. Privacy is extremely contextual, based in the specifics of by who, for what, where, why, and when a system is being used \parencite{ackerman2005privacy}. In that sense, individuals monitored and analyzed by the technology presented in this thesis might have divergent opinions regarding information privacy. Some individuals might believe the use of such technology is beneficial and it could be used to enhance their gaming experience, for instance. On the other hand, some individuals might oppose the use of such technology due to concerns about information privacy. \textcite{awad2006personalization} show that consumers of online shopping websites who desire greater information transparency are less willing to be profiled. As a counter-part, users that do want a more personalized experience in the online shopping are more willing to be profiled. One possible solution for such problem, which is a recommendation by \textcite{awad2006personalization}, is the utilization of mechanisms that account for both types of clients, the ones willing to be profiled to increase service personalization and those that are not.

The method proposed in this thesis is based on the analysis of video recordings. Normally users are not concerned about a video recording beyond the issue of the usage of their personal image. The present research, however, uses several techniques to collect additional data from those video recordings, including facial analysis and HR information. When being filmed during the interaction with a set of calibration games, a person might not be aware of the amount of information that is being actually collected. It is a matter of information privacy how such data is stored, processed and used. As previously mentioned, people are more prepared to accept potentially invasive technology if they consider that its benefits outweigh potential risks \parencite{ladd1991computers}. Users constantly decide and account for the trade-off between the benefit a solutions is giving and the privacy implications such act has. \textcite{nguyen2016effects} show that when privacy was evaluated against usability, convenience, and speed, the concern for privacy was relatively high. When compared against cost, concern for privacy was relatively low. It suggests that people have a clear trade-off between price/cost and privacy. As \textcite{awad2006personalization} demonstrated, some consumers of online shopping are willing to give personal information in exchange of better services. Consequentially users might be willing to be filmed and analyzed by a software to have a personalized emotion detector to improve game experience, for instance, specially if they see benefits in any existing trade-off analysis taking place. What needs to be clear to those users, however, is what data is being collected, by whom and how it will be used.

Technology is not neutral when it comes to privacy and it can increase or reduce the extent to which people have control over personal data \parencite{bellotti1993design}. The technology presented in this thesis, if used in misleading ways, contributes to reduce the control people have over personal data. It is paramount to ensure users know what is happening, which can be achieved with clear information practices. \textcite{langheinrich2001privacy} shows the principles for guiding system design, based on a set of fair information practices common in most privacy legislation in the use. The author highlights the following principles: notice, choice and consent, proximity and locality, anonymity and pseudonymity, security, and access and recourse. The principles of notice and choice and consent are essential to the technology presented here. Making users notice what is happening and what is being collected, as well as allowing choice and consent of the process is the bare minimum to ensure privacy.

\section{Ethics and privacy in this research}
\label{s:ethics-this-research}

The research presented in this thesis involved several elements that were affected by or have implications regarding ethics and privacy. The first of those elements is how the research was conducted. As presented in previous chapters, this research involved two experiments and several studies conducted on the data gathered from those experiments. All experiments, as well the aforementioned analysis and data collection, were conducted and handled according to the \textit{Principals for Research Ethics in the Humanities and Social Science}\footnote{\textit{Forskningsetiska Principer inom Humanistisk-Samh√§llsvetenskaplig Forskning}}. Particularly all materials and procedures were designed after recommendations of the CODEX\footnote{http://www.codex.vr.se}, the Swedish Research Council, which provides guidelines, ethics codes and laws that regulate and place ethical demands on the research process.

Following such guidelines, participants of the aforementioned experiments had given a written consent stating they understood what was happening and that they voluntarily wanted to participate in the study being conducted. Participants were informed that they may choose not to participate and they may withdraw their consent to participate at any time. In such case, they would not be asked for any explanations if they decide not to participate or to withdraw from any study. Additionally subjects were informed regarding the description of the research, including the overall research plan and the aim of the research being conducted. Data privacy issues were made clear to all participants, e.g. session being video-recorded, and all efforts were put in place to assure the participants privacy. For instance, participants were informed beforehand about which information was being collected from them, e.g. HR, video, in-game actions and answers to questionnaires. They were informed that the collected data would only be used for non-commercial research purposes, that all collected data would be stored off-line in a hard-drive that was only accessible by the principal investigator. Data protection included restrict access to such hard-drive, which was handled and keep physically under the principal investigator's possession at the dependencies of the University of Sk\"ovde. It helped ensure that unauthorized persons would not have access to the data. All participants were also informed beforehand that all data was anonymously collected and their identity would not be revealed, including in any publication resulting from this work. It is not possible to trace any published results or the information in the present thesis back to the subjects. Subjects were assured that their data would never be publicly disclosed, i.e. videos, HR information, pictures and answers to questionnaires, without their written consent. Some of the publications of this research presented pictures of participants, which were contacted in advance and asked for an authorization regarding the use such pictures.

Finally in order to ensure subjects fully understood the research being conducted and how their data would be used, in both experiments each subject attended a debriefing session. In such session, the researcher explained to subjects how the games just played by them were designed, including the linear progression of boredom and stress. Additionally subjects were informed regarding how the data would be analyzed, e.g. find a relation between HR and stressful moments, or that remote estimations of their HR would be performed on the video recordings and tested against the data collected by the watch participants used. Participants had the opportunity to ask questions about the experiment, the study, the data collection or any matter related to the research they deemed important. It is also important to highlight that a wearable sport device, i.e. watch (TomTom Cardio Runner), has been used in this research. Such device was selected in favor of a medical grade equipment to reduce privacy and ethical concerns regarding data gathering of personal biological information.

\section{Ethical and privacy implications of this research}
\label{s:ethics-this-research}

Ethical and privacy implications of the technology developed and presented in this thesis can be both discussed and speculated. Sections \ref{s:ethics-ethical} and \ref{s:ethics-privacy} previously presented a more formal and academic view of ethics, privacy and technology. This section presents a less formal and more personal discussion regarding the matter.

The technology presented in this thesis can help researchers and game makers to produce better games by tailoring experiences on a user level. The main goal is to help those actors and enhance the experience of the user, however it will inevitably result in several ethical issues. The quick pace in which technology progresses produces ever smaller devices for data collection, for instance cameras with infrared and depth sensors capturing data in the living room of players. Remote acquisition of physiological data applied to emotion detection, such as the method described in this thesis, allows dangerously easy access to the psychological profile of users. Even though companies and researchers mean no harm to subjects and customer, unethical use of any technology is facilitated when the process is completely remote and unobtrusive. A significant amount of personal information can now be acquired remotely from any users in any location, including HR and facial activity. Such information will certainly be used for a wide variety of aims, including personal and commercial gain. Differences in culture might play a big role in the acceptance and adoption of such technologies. In a society where the individual or freedom without limited regulation is valued, a technology that promises a tailored experience is likely to be accepted and not be seen as an invasion of privacy. However misuse of such technology might not even be anticipated or understood by all. For instance, it is quite common to accept that a particular interaction with an entity is being recorded, e.g. a phone call to a customer service center. What are the consequences of video recording a given meeting and later using the video data to infer emotional states? Is it fair or ethical to use such technology during a job interview to measure the reactions of a candidate when salaries are being discussed, so a value is reached to maximize the company's profit based on the emotional state of a person? Emotional exploitation of individuals becomes a plausible idea when no sensors need to be used to detect the emotional state, only an ordinary camera.

Measurement of emotions of players in a game could lead to pleasant and personalized experiences. However they could also lead to issues involving induced behavior to enforce malicious behavior, for instance. A company selling items can drastically benefit from any information regarding the emotional state of a user. If a particular user is more likely to spend money in game items or expansions when he/she is stressed, then the game can work to keep such player stressed more than other users. As previously mentioned, users are likely to accept a technology that they see as beneficial. The issue raises when users don't see the problems with a technology or the negative effects it might have. Consequentially users could be tricked into believing that a technology is good and works in their behalf, when the situation or the company intentions might be the complete opposite. In a world where all actions are guided by good will, there would be no need for reflections regarding the technology presented in this thesis. However good intentions are not always clear for different actors, so the technology presented in this thesis must be thoroughly discussed, regulated and monitored. Technological progress without regulations could lead to mass exploration of users without their consent. New technologies, specially those using remote acquisition of psychophysiological signals aimed for emotion detection, allow a new paradigm of data collection. Users might not even be aware of what can be remotely collected from them and used to infer emotions and induce a certain behavior. It is not unlikely to believe that users could be acting in a certain way believing they are following their free will, when in reality they are being manipulated based on their psychophysiological reactions. This is a situation that should never happen under ethical uses of any technology.

My personal opinion as a researcher regarding the work described in this thesis is that its benefits outweigh its problems. New tools and experiences can be created out of a better understanding of what the user is feeling during the interaction with a system. If used with privacy and ethics in mind, the technology proposed here can help the development of brand new categories of software and hardware that are not disconnected from its users emotions. Instead they are aware of them and can work on users behalf regarding that. Regulations and transparency towards data collection and its use is paramount to ensure an ideal use of this new technology.
