\chapter{Ethics and privacy}
\label{ch:ethics}

Several contributions of the research presented in this thesis have implications that concern ethics and privacy. Technology has become an essential part of modern life; therefore, the advances it brings to different fields, including human-computer interaction and games research, should be guided by ethics and privacy. This chapter provides an overview and discussion regarding how the research and the technology proposed in this thesis touches issues related to both ethics and privacy.

\section{Ethical use of technology}
\label{sec:ethics-ethical}

\textcite{mason1995applying} mentions that the facts of an ethical situation can be summarized by four factors. The first is the identification of the moral agent, which is the one causing the technology-induced change. The second relates to the available courses of action that the moral agent can undertake. It is not always possible, or even viable, to choose more than one course of action. Consequentially, it must be selected according to the best interests of all parties involved. Additionally, a course of action is bound to have consequences, which can be irreversible. In that light, the third factor to emerge is the delimitation of the results that are expected to occur if each act is taken. A proper delimitation of results makes it clear for the involved parties how to measure the impact and implications of an act. Finally, the fourth factor is the identification of the stakeholders who will be affected by the consequences of the acts.

One of the main goals of the technology developed in this thesis is a non-obtrusive form of emotion detection. Given that a person has agreed to have a user-tailored model of him/herself created, i.e. play the calibration games while being filmed, any moral agent, i.e. researcher or company, is then able to use such data freely and unrestrictedly. After the model has been trained, the person used to train said model can be indefinitely surveyed in a context of gaming. Once trained, the model can be easily transferred to another moral agent, e.g. another institution or company, and used at a later time. Even though the proposed method is constrained by a gaming context, it can still be widely used. If the person in question, who is the stakeholder of the process, is not properly and clearly informed about the identity of the moral agents and the delineation of the results expected from the use of his/her model, an ethical issue may exist.

An ethical issue is said to arise whenever one party in pursuit of its goals engages in behavior that materially affects the ability of another party to pursue its goals \parencite{mason1995applying}. One could claim that sharing a person's user-tailored model among institutions/companies is not materially affecting that person. Additionally, people are more prepared to accept potentially invasive technology if they consider that its benefits outweigh potential risks \parencite{ladd1991computers}. However, one of the moral agents might be a game development company that uses the model to detect the emotional state of a person, in order to maximize the sale of in-game goods. In that case, the act could materially affect the person, which would clearly be an ethical issue if the person was never made aware of such a possible use of his/her model. As previously mentioned, the facts of an ethical situation must be clear, otherwise obscure information about courses of action, delimitation of results and even the identity of the moral agents could lead stakeholders into making poor judgments regarding ethics and privacy.

Another implication of non-obtrusive technologies is how it influences the ability of a user to decline the propagation of any information. In the context of games research, for instance, if a subject answers a questionnaire about a game being played, it is completely plausible to assume that the subject could deliberately lie about the answers. Subjects might even decline to answer a particular question about emotions, if they feel uncomfortable, for instance. If the method proposed by this thesis is used to detect emotional states and one assumes that the subjects have previously agreed to the creation of a user-tailored model of themselves, then they do not have the option to decline to answer a query about emotions. A researcher might have a previously trained model of a subject, e.g. from an old experiment, which can be used again for the same subject, however in a different context. The method proposed in this thesis can be adapted to be trained on data from a group instead of an individual, i.e. group model instead of a user-tailored model. In that case, the trained model could be applied to any person (or subject), without the need for them to play the calibration games. It is plausible to believe that such a configuration of the method could be used by companies to survey a player's emotional responses to a particular game. A company could, for instance, apply the method to online videos, e.g. ``Let's play" videos on YouTube, to gather unsolicited emotional data. If the videos are freely available, does it mean such use of the method is ethical? Was the person in the video thinking about having his/her emotions automatically detected by a software when he/she made the video?

The technology proposed by this thesis has several limitations and constrains, however, it can be extended and improved to broaden its accuracy and usage. It has moral and ethical implications that should be discussed by all stakeholders involved, making the facts of any ethical situation completely clear and understood by all affected.

\section{Privacy and personal data}
\label{sec:ethics-privacy}

Discussions about privacy in the field of human-computer interaction are common and there is a clear indication that HCI tools must not invade a user's privacy \parencite{pantic2003toward}. Any tool's capacity to monitor and concentrate information about somebody's behavior must not be misused. The technology presented in this thesis significantly relates to privacy. As defined by \textcite{culnan2000protecting}, privacy is the ability of the individual to control the terms under which personal information is acquired and used. When a system collects personal information, which is the case of the method in this thesis, information privacy becomes an issue. \textcite{stone1983field} define information privacy as the ability of the individual to personally control information about one's self.

In the context of this thesis, information privacy relates to how a person controls the digital data collected from him/herself, e.g. video recordings and the user-tailored model. As previously mentioned, the technology presented in this thesis has moral and ethical implications, which leads to information privacy implications. Privacy is extremely contextual, based in the specifics of by whom, for what, where, why, and when a system is being used \parencite{ackerman2005privacy}. In that sense, individuals monitored and analyzed by the technology presented in this thesis might have divergent opinions regarding information privacy. Some individuals might believe the use of such technology is beneficial and could be used to enhance their gaming experience, for instance. On the other hand, some individuals might oppose the use of such technology, due to concerns about information privacy. \textcite{awad2006personalization} show that consumers using online shopping websites who desire greater information transparency are less willing to be profiled. In contrast, users that do want a more personalized experience when shopping online are more willing to be profiled. One possible solution for such a problem, which is a recommendation by \textcite{awad2006personalization}, is the utilization of mechanisms that account for both types of clients, the ones willing to be profiled to increase service personalization and those that are not.

The method proposed in this thesis is based on the analysis of video recordings. Normally, users are not concerned about a video recording beyond the issue of the usage of their personal image. The present research, however, uses several techniques to collect additional data from those video recordings, including facial analysis and HR information. When being filmed during the interaction with a set of calibration games, a person might not be aware of the amount of information that is actually being collected. How such data are stored, processed and used is a matter of information privacy. As previously mentioned, people are more prepared to accept potentially invasive technology if they consider that its benefits outweigh potential risks \parencite{ladd1991computers}. Users constantly decide and account for the trade-off between the benefit of a solution and the privacy implications of such act. \textcite{nguyen2016effects} show that when privacy was evaluated against usability, convenience, and speed, the concern for privacy was relatively high. However, when compared to cost, concern for privacy was relatively low. This suggests that people have a clear trade-off between price/cost and privacy. As \textcite{awad2006personalization} demonstrate, some consumers of online shopping are willing to give personal information in exchange for better services. Consequentially, users might be willing to be filmed and analyzed by a software, to have a personalized emotion detector to improve game experience, for instance, specially if they see benefits in any existing trade-off analysis taking place. What needs to be clear to those users, however, is the kind of data being collected, by whom and how it will be used.

Technology is not neutral, when it comes to privacy, and it can increase or reduce the extent to which people have control over personal data \parencite{bellotti1993design}. The technology presented in this thesis, if used in misleading ways, contributes to reducing the control people have over personal data. Ensuring users know what is happening, which can be achieved with clear information practices, is paramount. \textcite{langheinrich2001privacy} reveals the principles that guide system design, based on a set of fair information practices common in most privacy legislation in use. The author highlights the following principles: notice, choice and consent, proximity and locality, anonymity and pseudonymity, security, and access and recourse. The principles of notice, choice and consent are essential to the technology presented here. Making users notice what is happening and what is being collected, as well as allowing choice and consent of the process, is the bare minimum to ensure privacy.

\section{Ethical and privacy implications of this research}
\label{sec:ethics-implications}

One can both discuss and speculate about the ethical and privacy implications of the technology developed and presented in this thesis. Sections \ref{sec:ethics-ethical} and \ref{sec:ethics-privacy} present a more formal and academic view of ethics, privacy and technology. This section presents a less formal and more personal discussion regarding the matter.

The technology presented in this thesis can help researchers and game developers to produce better games by tailoring experiences at a user level. The main goal is to help those actors and enhance the user experience, which will, however, inevitably result in several ethical issues. The quick pace in which technology progresses produces ever smaller devices for data collection, for instance, cameras with infrared and depth sensors that capture data in the living room of players. The remote acquisition of physiological data applied to emotion detection, such as the method described in this thesis, allows dangerously easy access to the psychological profile of users. Even though companies and researchers mean no harm to subjects and customers, the unethical use of any technology is facilitated when the process is completely remote and unobtrusive. A significant amount of personal information can now be acquired remotely from any users in any location, including HR and facial activity. Such information will certainly be used for a wide variety of aims, including personal and commercial gain. Differences in culture might play a big role in the acceptance and adoption of such technologies. In a society where the individual and limited regulation are valued, a technology that promises a tailored experience will likely be accepted and not seen as an invasion of privacy. However, the misuse of such technology might not even be anticipated or understood by all. For instance, it is quite common to accept that a particular interaction with an entity is being recorded, e.g. a phone call to a customer service center. What are the consequences of video recording a given meeting and later using the video data to infer emotional states? Is it fair or ethical to use such technology during a job interview, to measure the reactions of a candidate on the basis of his/her emotional state when salaries are discussed, in order to reach an amount that maximizes the company's profit? The emotional exploitation of individuals becomes a plausible idea when no sensors need to be used to detect their emotional state, only an ordinary camera.

The measurement of players' emotions in a game could lead to pleasant and personalized experiences. However, it could also lead to issues involving induced behavior to enforce malicious behavior, for instance. A company selling items can significantly benefit from any information regarding the emotional state of a user. If a particular user is more likely to spend money on game items or expansions when he/she is stressed, then the game can work to keep such a player stressed more than other players. As previously mentioned, users are likely to accept a technology they regard as beneficial. The issue rises when users do not recognize the problems with a technology or the negative effects it might have. Consequentially, users could be tricked into believing that a technology is good and works on their behalf, when the situation or the company's intentions could be the complete opposite. In a world where all actions are guided by good will, there would be no need for reflections regarding the technology presented in this thesis. However, good intentions are not always clear to different actors, therefore, the technology presented in this thesis must be thoroughly discussed, regulated and monitored. Technological progress without regulations could lead to the mass exploration of users without their consent. New technologies, especially those using the remote acquisition of psychophysiological signals aimed for emotion detection, allow a new paradigm of data collection. Users might not even be aware of what can be remotely collected from them and used to infer emotions and induce a certain behavior. It is possible to believe that users could be acting in a certain way, in the belief they are following their own free will, when in reality they are being manipulated on the basis of their psychophysiological reactions. This is a situation that should never happen during the ethical uses of any technology.

My personal opinion as a researcher regarding the work described in this thesis is that its benefits outweigh its problems. New tools and experiences can be created from a better understanding of what the user is feeling during the interaction with a system. If used with privacy and ethics in mind, the technology proposed here can help the development of brand new categories of software and hardware that are not disconnected from their users emotions. Instead, they are aware of them and can work on the behalf of users in that regard. Regulations and transparency for data collection and its use are paramount to ensure the ideal use of this new technology.
