\chapter{Ethics and privacy}
\label{ch:ethics}

Several contributions of the research presented in this thesis have implications that concern ethics and privacy. Technology has become an essential part of modern life and the advances it brings to different fields, including human-computer interaction and games research, should be guided by ethics and privacy. This chapter provides an overview and discussion regarding how the technology proposed in this thesis touches issues related to both ethics and privacy.

\section{Ethical use of technology}

\textcite{mason1995applying} mentions that the facts of an ethical situation can be summarized by four factors. The first one is the identification of the moral agent, which is the one bringing the technology-induced change. The second relates to the available courses of action that the moral agent can undertake. It is not always possible, or even viable, to choose more than one course of action. Consequentially it must be selected according to the best interests of all parties involved. Additionally a course of action is bound to have consequences, which can be irreversible. In that light, the third factor emerges, which is the delineation of the results that are expected to occur if each act is taken. A proper delimitation of results make it clear for the involved parties how to measure impact and implications of an act. Finally the fourth factor is the identification of the stakeholders who will be affected by the consequences of the acts.

One of the main goals of the technology developed in this thesis is a non-obtrusive form of emotion detection. Given that a person has agreed to have a user-tailored model of him/herself created, i.e. play the calibration games while being filmed, any moral agent, i.e. researcher or company, is then able to use such data freely and unrestrictedly. After the model has been trained, the person used to train said model can be indefinitely surveyed in a context of gaming. Once trained, the model can be easily transfered to another moral agent, e.g. another institution or company, and used in a later time. Even though the proposed method is constrained by a gaming context, it can still be widely used. If the person in question, who is the stakeholder of the process, was not properly and clearly informed about who the moral agents are and the delineation of the results expected from the use of his/her model, a probable ethical issue exists.

An ethical issue is said to arise whenever one party in pursuit of its goals engages in behavior that materially affects the ability of another party to pursue its goals \parencite{mason1995applying}. One could claim that sharing a person's user-tailored model among institutions/companies is not materially affecting the person. Additionally, people are more prepared to accept potentially invasive technology if they consider that its benefits outweigh potential risks \parencite{ladd1991computers}. However, one of the moral agents might be a game developer company using the model to detect the emotional state of a person in order to maximize the selling of in-game goods. In that case, the act might be materially affecting the person. That is clearly an ethical issue if the person was never made aware of such possible use of his/her model. As previously mentioned, the facts of an ethical situation must be clear, otherwise obscure information about courses of action, delimitation of results and even who the moral agents are might lead stakeholders into making poor judgments regarding ethics and privacy.

Another implication of non-obtrusive technologies is how it influences the ability of user to decline the propagation of any information. In the context of games research, for instance, if a subject is answering a questionnaire about a game being played, it is completely plausible to assume that the subject could deliberately lie about the answers. Subjects might even decline to answer a particular question about emotions if they feel uncomfortable, for instance. If the method proposed by this thesis is being used to detect emotional states and assuming that subjects have previously agreed to have a user-tailored model of themselves created, subjects have no option to decline to answer a query about emotions. A researcher might have a previously trained model of a subject, e.g. from an old experiment, which can be used again for the same subject, however in a different context. The method proposed in this thesis can be adapted to be trained on data from a group instead of an individual, i.e. group model instead of a user-tailored model. In that case, the trained model could be applied to any person (or subject) without them having to play the calibration games. It is plausible to believe that such configuration of the method could be used by companies to survey player's emotional responses to a particular game. A company could, for instance, apply the method to on-line videos, e.g. ``Let's play" videos on YouTube, to gather unsolicited emotional data. If the videos are freely available, does it mean such use of the method is ethical? Was the person in the video thinking about having his/her emotions automatically detected by a software when he/she made the video?

The technology proposed by this thesis has several limitations and constrains, however it can be extended and improved to broaden its accuracy and usage. It has moral and ethical implications that should be discussed by all stakeholders involved, making the facts of any ethical situation completely clear and understood by all involved.

\section{Privacy and personal data}

Discussion about privacy in the field of human-computer interaction are common and there is a clear indication that HCI tools must not invade user's privacy \parencite{pantic2003toward}. Any tool's capacity to monitor and concentrate information about somebody's behavior must not be misused. The technology presented in this thesis significantly relates to privacy. As defined by \textcite{culnan2000protecting}, privacy is the ability of the individual to control the terms under which personal information is acquired and used. When a system collects personal information, which is the case of the method in this thesis, information privacy becomes an issue. \textcite{stone1983field} defines information privacy as the ability of the individual to personally control information about one's self.

In the context of this thesis, information privacy relates to how a person controls the digital data collected from him/herself, e.g. video recordings and the user-tailored model. As previously mentioned, the technology presented in this thesis has moral and ethical implications, which leads to information privacy implications. Privacy is extremely contextual, based in the specifics of by who, for what, where, why, and when a system is being used \parencite{ackerman2005privacy}. In that sense, individuals monitored and analyzed by the technology presented in this thesis might have divergent opinions regarding information privacy. Some individuals might believe the use of such technology is beneficial and it could be used to enhance their gaming experience, for instance. On the other hand, some individuals might oppose the use of such technology due to concerns about information privacy. \textcite{awad2006personalization} show that consumers of online shopping websites who desire greater information transparency are less willing to be profiled. As a counter-part, users that do want a more personalized experience in the online shopping are more willing to be profiled. One possible solution for such problem, which is a recommendation by \textcite{awad2006personalization}, is the utilization of mechanisms that account for both types of clients, the ones willing to be profiled to increase service personalization and those that are not.

The method proposed in this thesis is based on the analysis of video recordings. Normally users are not concerned about a video recording beyond the issue of the usage of their personal image. The present research, however, uses several techniques to collect additional data from those video recordings, including facial analysis and HR information. When being filmed during the interaction with a set of calibration games, a person might not be aware of the amount of information that is being actually collected. It is a matter of information privacy how such data is stored, processed and used. As previously mentioned, people are more prepared to accept potentially invasive technology if they consider that its benefits outweigh potential risks \parencite{ladd1991computers}. Users constantly decide and account for the trade-off between the benefit a solutions is giving and the privacy implications such act has. \textcite{nguyen2016effects} show that when privacy was evaluated against usability, convenience, and speed, the concern for privacy was relatively high. When compared against cost, concern for privacy was relatively low. It suggests that people have a clear trade-off between price/cost and privacy. As \textcite{awad2006personalization} demonstrated, some consumers of online shopping are willing to give personal information in exchange of better services. Consequentially users might be willing to be filmed and analyzed by a software to have a personalized emotion detector to improve game experience, for instance, specially if they see benefits in any existing trade-off analysis taking place. What needs to be clear to those users, however, is what data is being collected, by whom and how it will be used.

Technology is not neutral when it comes to privacy and it can increase or reduce the extent to which people have control over personal data \parencite{bellotti1993design}. The technology presented in this thesis, if used in misleading ways, contributes to reduce the control people have over personal data. It is paramount to ensure users know what is happening, which can be achieved with clear information practices. \textcite{langheinrich2001privacy} shows the principles for guiding system design, based on a set of fair information practices common in most privacy legislation in the use. The author highlights the following principles: notice, choice and consent, proximity and locality, anonymity and pseudonymity, security, and access and recourse. The principles of notice and choice and consent are essential to the technology presented here. Making users notice what is happening and what is being collected, as well as allowing choice and consent of the process is the bare minimum to ensure privacy.
