\chapter{Ethics and privacy}
\label{ch:ethics}

Several contributions of the research presented in this thesis have implications that concern ethics and privacy. Technology has become an essential part of modern life and the advances it brings to different fields, including human-computer interaction and games research, should be guided by ethics and privacy. This chapter provides an overview and discussion regarding how the technology proposed in this thesis touches issues related to both ethics and privacy. Finally there are some recommendations for researchers and practitioners regarding the use of results of this research.

\section{Ethical use of technology}

\textcite{mason1995applying} mentions that the facts of an ethical situation can be summarized by four factors. The first one is the identification of the moral agent, which is the one bringing the technology-induced change. The second relates to the available courses of action that the moral agent can undertake. It is not always possible, or even viable, to choose more than one course of action. Consequentially it must be selected according to the best interests of all parties involved. Additionally a course of action is bound to have consequences, which can be irreversible. In that light, the third factor emerges, which is the delineation of the results that are expected to occur if each act is taken. A proper delimitation of results make it clear for the involved parties how to measure impact and implications of an act. Finally the fourth factor is the identification of the stakeholders who will be affected by the consequences of the acts.

One of the main goals of the technology developed in this thesis is a non-obtrusive form of emotion detection. Given that a person has agreed to have a user-tailored model of him/herself created, i.e. play the calibration games while being filmed, any moral agent, i.e. researcher or company, is then able to use such data freely and unrestrictedly. After the model has been trained, the person used to train said model can be indefinitely surveyed in the context of games interaction. The model can be migrated to another moral agent, e.g. another institution or company, and used in a later time. Even though the proposed method is constrained by a gaming context, it can still be widely used. If the person in question, who is the stakeholder of the process, was not properly and clearly informed about who the moral agents are and the delineation of the results expected from the use of his/her model, a probable ethical issue exists.

An ethical issue is said to arise whenever one party in pursuit of its goals engages in behavior that materially affects the ability of another party to pursue its goals \parencite{mason1995applying}. One could claim that sharing a person's user-tailored model among institutions/companies is not materially affecting the person. Additionally, people are more prepared to accept potentially invasive technology if they consider that its benefits outweigh potential risks \parencite{ladd1991computers}. However, one of the moral agents might be a game developer company using the model to detect the emotional state of a person in order to maximize the selling of in-game goods. In that case, the act might be materially affecting the person. That is clearly an ethical issue if the person was never made aware of such possible use of his/her model. As previously mentioned, the facts of an ethical situation must be clear, otherwise obscure information about courses of action, delimitation of results and even who the moral agents are might lead stakeholders into making poor judgments regarding ethics and privacy.

Another implication of non-obtrusive technologies is how it influences the ability of user to decline the propagation of any information. In the context of games research, for instance, if a subject is answering a questionnaire about a game being played, it is completely plausible to assume that the subject could deliberately lie about the answers. Subjects might even decline to answer a particular question about emotions if they feel uncomfortable, for instance. If the method proposed by this thesis is being used to detect emotional states and assuming that subjects have previously agreed to have a user-tailored model of themselves created, subjects have no option to decline to answer a query about emotions. A researcher might have a previously trained model of a subject, e.g. from an old experiment, which can be used again for the same subject, however in a different context. The method proposed in this thesis can be adapted to be trained on data from a group instead of an individual, i.e. group model instead of a user-tailored model. In that case, the trained model could be applied to any person (or subject) without them having to play the calibration games. It is plausible to believe that such configuration of the method could be used by companies to survey player's emotional responses to a particular game. A company could, for instance, apply the method to on-line videos, e.g. ``Let's play" videos on YouTube, to gather unsolicited emotional data. If the videos are freely available, does it mean such use of the method is ethical? Was the person in the video thinking about having his/her emotions automatically detected by a software when he/she made the video?

The technology proposed by this thesis has several limitations and constrains, however it can be extended and improved to broaden its accuracy and usage. It has moral and ethical implications that should be discussed by all stakeholders involved, making the facts of any ethical situation completely clear and understood by all involved.

\section{Privacy and personal data}

Privacy is the ability of the individual to control the terms under which personal information is acquired and used \parencite{culnan2000protecting}.

Privacy is extremely contextual, based in the specifics of by who, for what, where, why, and when a system is being used. \parencite{ackerman2005privacy}

Information privacy is the ability of the individual to personally control information about one's self \parencite{stone1983field}.

Users differ widely in their privacy concerns.\parencite{ackerman2005privacy}

\textcite{awad2006personalization} show that consumers of online shopping websites who desire greater information transparency are less willing to be profiled. Authors recommend the utilization of mechanisms that account for both types of clients, the ones willing to be profiled to increase service personalization and those that are not. When non-obtrusive technologies and misinformation are combined, however, users might not

Technology is not neutral when it comes to privacy and it can increase or reduce the extent to which people have control over personal data \parencite{bellotti1993design}.

Discussion about privacy in the field of human-computer interaction are common and there is a clear indication that a HCI tools must not invade user's privacy; tool's capacity to monitor and concentrate information about somebody's behavior must not be misused. \parencite{pantic2003toward}

Attacker identity influences how people make trade-off for privacy. When privacy was evaluated against usability, convenience, and speed, concern for privacy was relatively high. When compared against cost, concern for privacy was relatively low (people more concerned about price/cost) \parencite{nguyen2016effects}.

\section{Recommendation for researchers and practitioners}

Principles for guiding system design, based on a set of fair information practices common in most privacy legislation in the use: notice, choise and consent, proximity and locality, anonymity and pseudonymity, security, and access and recourse \parencite{langheinrich2001privacy}.
