\chapter{Future work}
\label{ch:closing}

This chapter describes the plan of tasks to be performed in order to complete the proposed research aim. The tasks are related to the research objectives. Figure \ref{fig:future-work-objectives} illustrates the parts and the research objectives whose tasks will be conducted.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/future-work-objectives.png}
    \caption{Highlight of research objectives and related parts of the solution whose tasks will be conducted as future work.}
    \label{fig:future-work-objectives}
\end{figure}

The tasks involve the refinement of the process of remote acquisition of signals, definition of inputs for the user-tailored model, investigation of machine learning techniques, execution of an experiment involving emotion detection and finally the construction of a software that uses and validates the proposed method. Table \ref{tab:schedule} illustrates the schedule regarding the progression of the tasks. The following sections describe the tasks in detail.

\begin{landscape}

\begin{table}
\caption{Schedule of planned tasks}
\label{tab:schedule}
\centering
\resizebox{1.45\textwidth}{!}{%
\begin{tabular}{|p{8.3cm}|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{Activity} & \multicolumn{8}{|c|}{2017} & \multicolumn{12}{|c|}{2018} & \multicolumn{2}{|c|}{2019} \\
\hline
& 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 1 & 2 \\
\hline
Refinement of remote acquisition of signals & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & & & & & & & & & & & & & & & & & & \\
\hline
Definition of inputs for the user-tailored model & & & & \cellcolor{Gray} & \cellcolor{Gray} & & & & & & & & & & & & &  & & & & \\
\hline
Investigation of machine learning techniques & & & & & & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & & & & & & & & & & \\
\hline
Experiment involving emotion detection & & & & & & & & & & & & & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & & & & & & & \\
\hline
Development of software to validate the approach & & & & & & & & & & & & & & & & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & & \\
\hline
Thesis writing & & & & & & & & \cellcolor{Gray} & & & & \cellcolor{Gray} & & & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & \cellcolor{Gray} & & \\
\hline
Disputation & & & & & & & & & & & & & & & & & & & & & & \cellcolor{Gray} \\
\hline
\end{tabular}%
}
\end{table}

\end{landscape}

\section{Refinement of remote acquisition of signals}

The information presented in chapter \ref{ch:literature-rppg} suggests that techniques for remote extraction of physiological signals of users are significantly affected by natural behavior, e.g. head movement and facial activity. The results of the first experiment also support that, indicating that users indeed behave in a way that directly affect the accuracy of measurements. Preliminary analysis \parencite{bevilacqua2017accuracy} suggests that the accuracy of the rPPG technique is feasable under such circustances, however it is still not clear how the accuracy problems interfer with a predictive model.

As a consequence, studies will be conducted to investigate how the selected rPPG technique is affected by natural behavior of user and how it can be improved to mitigate or elimitate those problems. Figure \ref{fig:rppg-accuracy-study} illustrates the process to be conducted to evaluate and improve the accuracy of the rPPG technique.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/rppg-accuracy-study}
    \caption{Process to evaluate the accuracy of HR estimations performed by rPPG when influenced by natural behavior.}
    \label{fig:rppg-accuracy-study}
\end{figure}

The video recordings of the first experiment will be used in the analysis. Each video will be analysed using a moving window of size $N$ seconds, which will be moved along the video with a 1 second offset, producing a set $S$ of video segments $s_i$ with $N$ seconds each. For example, assuming a 1 minute long video and a 30 seconds moving window ($N=30$), the analysis process will result in 30 $s_i$ segments ($S=30$) of 30 seconds each (0 to 30, 1 to 31, ..., 29 to 59).

Each segment $s_i$ will be used as the input for the rPPG technique for HR estimation. The estimated HR value will be compared to the average HR calculated from ground truth for the duration of the segment $s_i$. Additionally facial movement information will be calculated for each segment, such as variations of ROI size, ROI position the and variations of the central point of the detected face. Similar calculations were already conducted in study 3 (section \ref{s:study3}).

Different values for the window size $N$ will be used in the analysis, which will identify a correlation among the size of $N$, the effect of user movements and the accuracy of HR estimations. Those values will guide the adaptations proposed by previous work \parencite{li2014remote} to be applied into the selected rPPG technique to improve its accuracy within the context of this research. Examples of addaptations include the utilization of only video segments with lower subject motion, average HR estimations among different segments to infer the current HR, among others.

\section{Definition of inputs for the user-tailored model}
\label{sec:closing-definition-inputs}

The literature review presented in chapters \ref{ch:literature-physiological}, \ref{ch:literature-face} and \ref{ch:literature-multifactorial} indicates that a model based on several user signals, which is a multifactorial analysis, is more efficient for emotion detection. The mentioned chapters also highlight which of those signals can be remotely acquired within the context of this research via computer vision techniques.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/model-inputs-set.png}
    \caption{Overall structure of the user-tailored emotion detection model regarding input (user signals) and output (stress/boredom levels).}
    \label{fig:model-inputs-set}
\end{figure}

The user-tailored model proposed for this research might have $N$ input signals, varying from physiological ones, e.g. HR, to non-physiological ones, e.g. facial actions and head movements. Figure \ref{fig:model-inputs-set} illustrates the overall structure of the model. In order to be used in the model, an input signal needs to be supported by previous work regarding emotion detection, as well as be validated within the process of the proposed game-based calibration phase. Time and scope constraints limit the amount of input signals that can be implemented, evaluated and used in this research. As a consequence, a study will be conducted to investigate, validate and initially implement two of those signals into the proposed model: HR and facial activity (which includes head movement, lips activity, etc).

The techniques and works presented in chapter \ref{ch:literature-face}, which relate to face detection and emotion estimation, suggest that facial analysis is an important component of a multifactorial emotion detection model. Empirical analysis of the data from the first experiment also suggest that individualities regarding facial activities do exist and could be used to estimate emotional states on a user-tailored basis \parencite{bevilacqua2016variations}. As described in section \ref{ch:literature-face-emotion-prediction}, facial actions, head movement, lips/eye/mouth activity and distance measurements of detected facial landmarks are viable and proven sources of information for emotion detection.

Regarding physiological signals, results indicate that the average HR mean for players during the last minute of gameplay is greater than the average HR mean during the second minute of gameplay. The findings are aligned with and reinforce previous research that indicate higher HR mean during stressful situations in a gaming context. The findings also suggest that changes in the HR during gaming sessions is a promising indicator of stress.

The study will involve the definition of how those two signals will be used as inputs for the model. Facial actions, for instance, will probably be detected and measured by the euclidian distance of the facial landmarks. A vector containing the distances will be evaluated as the input for the model. Regarding the HR, its mean and standard variation during a particular analysis window will be evaluated as input for the model. A software for the detection of those two signals will be created and used to analyse the video recordings of the first experiment. The inclusion or exclusion of a component of a signal, e.g. variations of the distances of the lips landmark points, will be based on the accuracy to detect them and the frequency they appear in boring and stressful part of the calibration games.

\section{Investigation of machine learning techniques}
\label{closing:investigation-machine-learning}

The majority of the previous work found in the literature mention the use of machine learning techniques to model user signals into emotional states. Different models and accuracy results are mentioned, which dependent on several particularities of the approach used by the authors. A machine learning model will also be used by this research as a user-tailored emotion detection model.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/machine-learning-investigation.png}
    \caption{Iteration of a 3 fold cross validation performed on the video of a gaming sessions with 3 games, i.e. A, B and C. The videos of two calibration games, e.g. A and B, are used to train the machine learning model, while the video of the third calibration game, e.g. C, is used to test the model.}
    \label{fig:machine-learning-investigation}
\end{figure}

A systematic study and accuracy evaluation will be performed to select the proper machine learning technique to be used in the model. The evaluation process will be conducted on each one of the selected (and competing) machine learning techniques using the video recordings of experiment 1. The evaluation is based on a 3 fold cross validation process, illustrated in Figure \ref{fig:machine-learning-investigation}. Initially the input signals for the emotion detection model (defined in the previous tasks, section \ref{sec:closing-definition-inputs}) will be extracted from two, e.g. A and B, of the three games played by a user and used to train the emotion detection model. The thrid game. e.g. C, that was left out of the training will be used as a testing set: user signals will be extracted from the video of that game and fed into the trained emotion detection model, which will output the predicted emotional state of the user. The 3 fold cross validation process is repeated three times, each one of them leaving out of the training phase a different game, i.e. A and B are used for training and C is used for testing, A and C are used for training and B for testing, and finally B and C are used for training and A for testing.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/machine-learning-labeling-approach-A.png}
    \caption{Labeling approach for the training of the user-tailored model based on a fixed point of division $P$ for both boredom and stress samples.}
    \label{fig:machine-learning-labeling-approach-A}
\end{figure}

The video recordings that will be used in the tasks are related to experiment 1, whose games were designed to work as calibration games. As previously described in section \ref{sec:research-aim}, those calibration games feature a progression from a boring to a stressful state. That configuration will be used as the foundation for the labeling process of emotional states during the training of the model, as well as ground truth for its testing.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/machine-learning-labeling-approach-B.png}
    \caption{Labeling approach for the training of the user-tailored model based on varying points of division: $P_b$ (boredom samples) and $P_s$ (stress samples).}
    \label{fig:machine-learning-labeling-approach-B}
\end{figure}

The training of the emotion detection model will be conducted according to two different strategies. In both approaches, the two games used as training sets will have their user signals sampled at a fixed interval of $T$ seconds, being $T$ empirically defined. The labeling of a sample as boredom or stress, however, will be different. In approach \textbf{A}, illustrated in Figure \ref{fig:machine-learning-labeling-approach-A}, assuming that $P$ represents the time progression of each game, e.g. $P=0$ is the starting point of the game and $P=1$ is its end point, each game will be divided in two equal parts ($P=0.5$). Samples from the first half of the game will be labeled as boredom, while samples from the second half will be labeled as stress. The division is based on the assumption that the middle of the games accurately separates in time the self-reported perceptions of boredom and stress made by the subjects. In approach \textbf{B}, illustrated in Figure \ref{fig:machine-learning-labeling-approach-B}, the self-reported levels of boredom and stress associated with the beginning and the end of each game will be used. Assuming that $S_i$ and $B_i$ represent the self-reported stress and boredom levels of a given subject, respectively, where $i=0$ represents the beginning part of a game, and $i=1$ represents the ending part of a game. Two distinc division points will be calculated for boredom and stress labelings. The division point for stress, $P_s$, will be calculated as:

\begin{equation}
P_s = 1 - \frac{S_1 - S_0}{5}
\end{equation}

and the division point for boredom, $P_b$, will be calculated as:

\begin{equation}
P_b = \frac{B_1 - B_0}{5}
\end{equation}

where 5 is the maximum value allowed for the self-reported answers. The values of $P_s$ and $P_b$ will range within the interval [0,1], which will be used as division points similarly to the one described for approach \textbf{A}. For example, a subject that reported $S_0=1$ and $S_1=4$ will have $P_s=0.4$, while a subject with $S_0=3$ and $S_1=4$ will have $P_s=0.8$. Samples from the second half of the division $P_s$ will be labeled as stress, while the samples from the first half of $P_b$ will be labeled as boredom. Approach \textbf{B} tries to mitigate the assumption that the middle point of the games perfectly divides the perceptions of boredom and stress. The approach accounts for the informed levels of boredom and stress of the subject, labeling only the samples within the areas more likely to accurately reflect the self-reported emotional states.

The testing process will be similar for both approaches \textbf{A} and \textbf{B}. Samples from the game used as a testing set will be collected at a fixed internal of $K$ seconds, which will be larger than $T$ and also defined empirically. The associated labeling of the samples will be based on their position in the two parts calculated according to the rules of division points for each approach. Sample points that eventually are not labeled, e.g. middle points in approach \textbf{B}, will be labeled as neutral.

Following the described procedure, after all machine learning techniques are tested, they will have several resulting accuracy scores, i.e. $3 \cdot K \cdot 20$, where 20 is the number of subjects of experiment 1. The technique with the highest mean for the accuracy score will be selected. The following machine learning and classification techniques will be initially used in the tests: Support Vector Machine (SVM) using a radial basis, C-Support Vector Classification (C-SVC) using a linear kernel, K-nearest neighbours (K-nn), AdaBoost using nearest mean classifiers, Naive Bayers, and neural networks probably represented by convolution networks (convnets). Previous work also suggest a process involving decision fusion or a hierarchy of two or more classifiers working on different feature sets to improve prediction rates. Those approaches will probably be investigated as well.

\subsection{Challenges and unresolved issues}

One of the main unresolved issues regarding the use of a machine learning model is regarding its input features. Computer vision will be used to remotely extract a set of signals from the users, e.g. HR and the position of facial landmarks, however how those signals will be packeged as inputs for the machine learning model is still undefined.

The current idea is to use a direct and discrete approach where the set of extracted signals from each frame of the video being analyzed is used as is. This approach does not explicitly feed the model with information regarding the variations of the extracted signals, e.g. HR decreased 5 bpm in the last 10 seconds, instead current information from each frame is used as input, e.g. HR is 60 bpm now. The approach relies on the principle that the machine learning model will recognize any patterns regarding the variation of signals among the different samples over time, modeling those patterns as the desired mapping of emotional states.

Another possible solution is to provide the model with the variations of the signals, which demands a pre-processing of the extracted signals before feeding them into the model. In such scenario, each extracted signal will be accompanied by additional data, e.g. standard deviation, mean, etc. Ideally the machine learning model will better account for the variations of the signals and the emotional states. If a convolutional neural network is used as a machine learning solution, for instance, each signal can be used as input to the model in the form of a matrix. In that case each row of the matrix contains a segment of the signal at different times and the convnet will automatically find a way to model such changes into emotional states.

\section{Experiment involving emotion detection}
\label{closing:emotion-detection-experiment}

An experiment involving emotion detection and games will be planned and executed to validate the proposed approach. The experiment will be similar to the first one conducted, however it will test the hypotesis that all defined components, i.e. computer vision technique, machine learning model and calibration games, work in combination to detect emotional states.

The experiment design will be based on a within-subject approach \parencite{lane2015online} where all participants perform at all levels of the treatment and there are no control groups. In the context of this research, user signals, e.g. HR, facial actions and self-reported emotional state, will be measured and used in a user-tailored model, so the division of subjects into more than one group poses a comparison problem. Each individual will inevitably differ from one another regarding signals and emotions, such as variations in average HR during rest, for instance. Additionally the emotional state of users regarding boredom and stress must be measured constantly during the experiment, so the accuracy of the user-tailored model can be tested against those points where the emotional state is known. People present different perceptions regarding stress and boredem, so a division of subjects into groups also poses comparison problems.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/time-series-design-breakwell.png}
    \caption{Time-series experimental design using an A-B-A (baseline, treatment, baseline) approach. Reproduced from \textcite{breakwell1994research}.}
    \label{fig:time-series-design-breakwell}
\end{figure}

Since the emotional state of users must be measured constantly, a time-series experiment design will be used. In a time-series design there is a periodic measurement process on an individual and the introduction of an experimental change into this time series of measurements, which results in a discontinuity in the measurements recorded in the time series \parencite{campbell2015experimental}. Figure \ref{fig:time-series-design-breakwell} illustrates the design. The A-B-A design is a common single-case experimental design in which the measurements are conducted throughout the three parts of the experiment, i.e. A, B and A. Phase A, referred to as the baseline phase, is a period where the subject is not under the effect of the treatment, so the measurements should reflect natural occurences. Phase B, referred as the treatment phase, is the period where the treatment/intervention is applied. In the A-B-A design, the application of a treatment followed by its removal should result in changes in the measurements among the three phases, e.g. lower values during phase A and elevated values during phase B, which confirms that the variation is a result of the treatment.

The experiment will use a multiple treatment design which involves the implementation of two or more treatments designed to affect a single behavior \parencite{robson2016real}. It will have the form of A-B-A-C-A-D-A, where B, C and D are treatments. Figure \ref{fig:closing-experiment2-design} illustrates the design of the experiment. Treatments B and C will be calibration games, i.e. $G_i$, while treatment D will be an ordinary off the shelf game, i.e. $H$. .

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/closing-experiment2-design.png}
    \caption{Time series experimental design used in experiment 2. $S_j$ represents the $j^{\text{th}}$ subject, $G_i$ are calibration games, $H$ is an off the shelf game, and $rest$ is a resting period.}
    \label{fig:closing-experiment2-design}
\end{figure}

Two calibration games, named $G_1$ and $G_2$, will be selected as treatments. The order they will be played will be randomized among participants, so one subject will have $G_1$ for treatment B and $G_2$ for treatment C, while another subject will have $G_2$ for B and $G_1$ for C, and so on. The measurement to be conducted will be regarding the level of stress and boredom. At fixed intervals, e.g. every 60 seconds, users will be asked to report the current emotional state. The measurement mechanism will be one of the following options. A likert scale containing a question about the stress level and a question about the boredom level. The Self-Assessment Manikin (SAM) \parencite{morris1995observations}, which is an efficient cross-cultural measurement of emotional response regarding valence and arousal. Or the Affective Slider (AS) \parencite{betella2016affective}, a digital self-reporting tool composed of two slider controls for assessment of pleasure and arousal. Figure \ref{fig:sam-as} illustrates SAM and AS self-assessment mechanisms.

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/SAM.png}
    \caption{}
    \label{fig:sam}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/AS.png}
    \caption{}
    \label{fig:as}
  \end{subfigure}
  \caption{Self-reporting meaurement of emotional states. (a) Self-Assessment Manikin (SAM) \parencite{morris1995observations}; (b) Affective Slider (AS) \parencite{betella2016affective}.}
  \label{fig:sam-as}
\end{figure}

Additionally to the self-assessment of the emotional state, during the whole experiment subjects will be recorded by a video camera and monitored by a HR watch. The data collected during the experiment regarding the calibration games will be used to train a machine learning model, which will be used to detect the emotional state of users during the interaction with the ordinary game. The processing will be performed offline and after the experiment. Results of that analysis will prove or refute the previously mentioned hypothesis that all defined components, i.e. computer vision technique, machine learning model and calibration games, work in combination to detect emotional states.

%When measuring HR, for instance, some subjects will have higher/lower HR mean than others, independent of the group they are in or the treatment they undergo. To counter that problem, the experiment will use a one-group posttest design \cite{kirk1982experimental}, as illustrated by Figure \ref{fig:closing-experiment2-design}. Using the first row as an example, subject $S_0$ played game $G_a$ as the first level of the treatment, followed by a post-test of that game ($PT_a$), then a rest period. In the second level of the treatment, the subject played game $G_b$, followed by a post-test of that game ($PT_b$), then another rest period. Finally in the third level of the treatment, the subject played game $G_c$ followed by a post-test of that game ($PT_c$).

%By using a one-group posttest design, each individual will perform on all levels of the treatment (play a set of different games). The within-subjects approach ensures that the differences between subjects are not interfering in the comparison, since a subject is being compared to his/herself in the different levels of the treatment. Subjects are not being compared among each other. In essence, each subject is serving as his/her own control group. According to Kirk \cite{kirk1982experimental}, the one-group posttest design should only be used when the researcher knows the mean value of the independent variable when no treatment is in effect. Such information will be obtained during the resting periods of the experiment, where the baseline value for all measured signals can be established for each subject.

%The process of sampling a group of participants for each experiment will follow the convenience sampling approach, a non-probability sampling technique where participants are recruited because of their convenient accessibility/proximity to the researcher. Volunteers will be randomly recruited for each experiment. A probability sampling approach, where each individual of the population has an equal chance of being selected, would be ideal and would strength the external validity of the research. However the costs, logistics and time constraints associated with it makes such approach impractical in the context of this research.

\section{Software deployment}

Finally when the emotion detection process is tested and validated, it will be deployed as a technological solution in the form of a software. All previously described tasks involve the implementation of algorithms to remotely extract the signals and to classify them using machine learning. All those steps will be coded in Matlab to provide fast iteration and to ensure scientific valididy with previous work.

This tasks involves the translation of all those developed parts, i.e. computer vision extraction of signals, processing of signals and mapping using machine learning, into a usable software. Since each of those parts have already been validated, the software can be seen as an encapsulation of those parts, which results in a valided usable solution. The software will be developed using OpenCV and C++.

%Questionnaires and physiological measurements are the most common approach used to obtain data for emotion estimation in the field of HCI and games research. Both approaches interfere with the natural behavior of users, which affects any research procedure. Initiaties based on computer vision and remote extraction of user signals for emotion estimation exist, however they are limited. Experiments of such initiatives were performed under extremely controlled situations with few game-related stimuli. Users had a passive role with limited possibilities for interaction or emotional involvement, differently than game-based emotion stimuli, where users take an active role in the process, making decision and directly interacting with the media. Previous works also focus on predictive models based on a group perspective. As a consequece, a model is usually trained from data of several users, which in practice describes the average behavior of the group, excluding or diluting key individualities of each user. In that light, there is a lack of initiatives focusing on non-obtrusive, user-tailored emotion detection models, in particular regarding stress and boredom, within the context of games research that are based on emotion data generated from game stimuli.

%After the rPPG technique has been improved and the facial analysis has been structured, a fourth study will be conducted on the data of the first experiment. It will guide the process of refining the remote detection of physiological signals along with facial analysis, which is the foundation for the predictive model proposed in this thesis. At the present moment, the software required to perform such study is almost finished and ready to be deployed. The results of this study should lead to another publication, which will describe the improvements applied to the rPPG technique and how they affect remote estimation of physiological signals.
